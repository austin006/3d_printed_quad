version: '3.8'

services:
  # Ollama service for LLM
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]  # Optional: for GPU support
    command: serve
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Model loader - ensures model is available
  model-loader:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    command: pull llama3.2:3b  # Change to tinyllama for RPi4
    network_mode: "service:ollama"

  # Drone control agent
  drone-agent:
    build: .
    container_name: drone-control-agent
    depends_on:
      model-loader:
        condition: service_completed_successfully
    environment:
      - ROS_DOMAIN_ID=0
      - RMW_IMPLEMENTATION=rmw_cyclonedds_cpp
    network_mode: host  # Required for ROS2 discovery
    stdin_open: true
    tty: true
    volumes:
      - ./config.yaml:/drone_agent/config.yaml:ro  # Mount config for easy updates
    command: /bin/bash -c "source /opt/ros/jazzy/setup.bash && python3 agent.py"

  # Note: For integration with existing simulation, you may want to comment out
  # the drone-agent service and run it manually after starting your simulation
  
volumes:
  ollama_data:

# Usage:
# 1. Start all services: docker-compose up -d
# 2. View logs: docker-compose logs -f drone-agent
# 3. Interact with agent: docker attach drone-control-agent
# 4. Stop all: docker-compose down

# For Raspberry Pi 4:
# - Change model to tinyllama in model-loader
# - Remove GPU reservation in ollama service
# - Consider using docker-compose --compatibility up for resource limits